{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ELUC       = '/GCB2021/Data/Output_GCB2021/202109_GCB2021_run/run_v1_1700-2021/'\n",
    "dir_ctrs       = '/Trendy/Data/data_ancillary/info_countries/'\n",
    "dir_grids      = '/Trendy/Data/grids/'\n",
    "dir_ELUC_2021  = '/GCB2021_commentary/Data/ELUC_countries/'\n",
    "dir_ELUC_NGHGI = '/GCB2021_commentary/Data/data_ELUC_NGHGI/'\n",
    "dir_peat       = '/GCB2021/Data/Peat_data/'\n",
    "dir_tmp        = '/Trendy/Data/tmp/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate gridded ELUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select time\n",
    "time_sta = '2000'\n",
    "time_end = '2020'\n",
    "\n",
    "#Define necessary variables\n",
    "variables = ['CD_A', 'CD_A_c', 'CD_A_p', 'CD_A_dec_h', 'CD_A_deg_d', 'CD_A_a', 'CD_A_reg_h', 'CD_A_deg_rec_d', 'cell_area']\n",
    "\n",
    "#Read data\n",
    "file_2021_HIS = dir_ELUC + '202109_GCB2021_run_v1_1700-2021__CurrentCPools_CD_PerHistoryType.nc'\n",
    "file_2021_COV = dir_ELUC + '202109_GCB2021_run_v1_1700-2021__CurrentCPools_CD_PerCoverType.nc'\n",
    "file_2021_SUM = dir_ELUC + '202109_GCB2021_run_v1_1700-2021__CurrentCPools_CD.nc'\n",
    "data_2021_HIS = xr.open_dataset(file_2021_HIS)\n",
    "data_2021_COV = xr.open_dataset(file_2021_COV)\n",
    "data_2021_SUM = xr.open_dataset(file_2021_SUM)\n",
    "\n",
    "#Select in time period\n",
    "data_2021_HIS = data_2021_HIS.sel(time=slice(time_sta, None))\n",
    "data_2021_COV = data_2021_COV.sel(time=slice(time_sta, None))\n",
    "data_2021_SUM = data_2021_SUM.sel(time=slice(time_sta, None))\n",
    "\n",
    "#Select variables to extract for PerHistoryType file\n",
    "variables_drop_HIS = [variab for variab in data_2021_HIS.data_vars if variab not in variables]\n",
    "variables_drop_COV = [variab for variab in data_2021_COV.data_vars if variab not in variables]\n",
    "variables_drop_SUM = [variab for variab in data_2021_SUM.data_vars if variab not in variables]\n",
    "\n",
    "data_2021_HIS = data_2021_HIS.drop(variables_drop_HIS)\n",
    "data_2021_COV = data_2021_COV.drop(variables_drop_COV)\n",
    "data_2021_SUM = data_2021_SUM.drop(variables_drop_SUM)\n",
    "\n",
    "data_sources = data_2021_COV['CD_A_c'] + data_2021_COV['CD_A_p'] + data_2021_HIS['CD_A_dec_h'] + data_2021_HIS['CD_A_deg_d']\n",
    "data_sinks   = data_2021_HIS['CD_A_a'] + data_2021_HIS['CD_A_reg_h'] + data_2021_HIS['CD_A_deg_rec_d']\n",
    "data_net     = data_2021_SUM['CD_A']\n",
    "\n",
    "#Calculate ELUC\n",
    "BLUE_ELUC_sources = data_sources.diff('time')\n",
    "BLUE_ELUC_sinks   = data_sinks.diff('time')\n",
    "BLUE_ELUC_net     = data_net.diff('time')\n",
    "BLUE_ELUC_sources['time'] = BLUE_ELUC_sources['time'] - 1\n",
    "BLUE_ELUC_sinks['time']   = BLUE_ELUC_sinks['time'] - 1\n",
    "BLUE_ELUC_net['time']     = BLUE_ELUC_net['time'] - 1\n",
    "BLUE_ELUC_sources = BLUE_ELUC_sources.sel(time=slice(time_sta, time_end))\n",
    "BLUE_ELUC_sinks   = BLUE_ELUC_sinks.sel(time=slice(time_sta, time_end))\n",
    "BLUE_ELUC_net     = BLUE_ELUC_net.sel(time=slice(time_sta, time_end))\n",
    "\n",
    "#Convert to dataset and set units\n",
    "BLUE_ELUC_sources = BLUE_ELUC_sources.to_dataset(name='ELUC')\n",
    "BLUE_ELUC_sinks   = BLUE_ELUC_sinks.to_dataset(name='ELUC')\n",
    "BLUE_ELUC_net     = BLUE_ELUC_net.to_dataset(name='ELUC')\n",
    "BLUE_ELUC_sources.ELUC.attrs['unit'] = 'Tg C/year'\n",
    "BLUE_ELUC_sinks.ELUC.attrs['unit']   = 'Tg C/year'\n",
    "BLUE_ELUC_net.ELUC.attrs['unit']     = 'Tg C/year'\n",
    "\n",
    "#Define compression level\n",
    "comp = dict(zlib=True, complevel=2)\n",
    "\n",
    "#Save in file\n",
    "fname_out_sources = dir_ELUC_2021 + 'ELUC_BLUE_GCB2021_ELUC-sources-density_' + time_sta + '-' + time_end + '.nc'\n",
    "fname_out_sinks = dir_ELUC_2021 + 'ELUC_BLUE_GCB2021_ELUC-sinks-density_' + time_sta + '-' + time_end + '.nc'\n",
    "fname_out_net = dir_ELUC_2021 + 'ELUC_BLUE_GCB2021_ELUC-net-density_' + time_sta + '-' + time_end + '.nc'\n",
    "if not os.path.exists(fname_out_sources):  BLUE_ELUC_sources.to_netcdf(fname_out_sources, encoding={var: comp for var in BLUE_ELUC_sources.data_vars})\n",
    "if not os.path.exists(fname_out_sinks):    BLUE_ELUC_sinks.to_netcdf(fname_out_sinks, encoding={var: comp for var in BLUE_ELUC_sinks.data_vars})\n",
    "if not os.path.exists(fname_out_net):      BLUE_ELUC_net.to_netcdf(fname_out_net, encoding={var: comp for var in BLUE_ELUC_net.data_vars})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read ELUC BLUE (GCB2021, without peat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sta = 2000\n",
    "time_end = 2020\n",
    "time_len = time_end - time_sta + 1\n",
    "\n",
    "#Read ELUC data and peat data\n",
    "fname_BLUE_snk = dir_ELUC_2021 + 'ELUC_BLUE_countries-ISOcode_ELUC-sinks_vRemapCountries_2000-2020.xlsx'\n",
    "fname_BLUE_src = dir_ELUC_2021 + 'ELUC_BLUE_countries-ISOcode_ELUC-sources_vRemapCountries_2000-2020.xlsx'\n",
    "ELUC_BLUE_snk = pd.read_excel(fname_BLUE_snk, header=0, index_col=0)\n",
    "ELUC_BLUE_src = pd.read_excel(fname_BLUE_src, header=0, index_col=0)\n",
    "\n",
    "#Select time\n",
    "ELUC_BLUE_snk = ELUC_BLUE_snk.loc[(ELUC_BLUE_snk.index>=time_sta) & (ELUC_BLUE_snk.index<=time_end)]\n",
    "ELUC_BLUE_src = ELUC_BLUE_src.loc[(ELUC_BLUE_src.index>=time_sta) & (ELUC_BLUE_src.index<=time_end)]\n",
    "\n",
    "#Check time selection\n",
    "if (ELUC_BLUE_snk.index[0]!=time_sta) | (ELUC_BLUE_snk.index[-1]!=time_end) | (len(ELUC_BLUE_snk.index)!=time_len):  sys.exit('Check time selection of BLUE!')\n",
    "if (ELUC_BLUE_src.index[0]!=time_sta) | (ELUC_BLUE_src.index[-1]!=time_end) | (len(ELUC_BLUE_src.index)!=time_len):  sys.exit('Check time selection of BLUE!')\n",
    "\n",
    "# UNITS: Tg C / year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read ELUC OSCAR (GCB2021, without peat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read ELUC data and peat data\n",
    "fname_OSCAR_snk = dir_ELUC_NGHGI + 'OSCAR_ELUC-sinks_GCB2021_countries.xlsx'\n",
    "fname_OSCAR_src = dir_ELUC_NGHGI + 'OSCAR_ELUC-sources_GCB2021_countries.xlsx'\n",
    "ELUC_OSCAR_snk = pd.read_excel(fname_OSCAR_snk, header=0, index_col=0)\n",
    "ELUC_OSCAR_src = pd.read_excel(fname_OSCAR_src, header=0, index_col=0)\n",
    "\n",
    "#Select time\n",
    "ELUC_OSCAR_snk = ELUC_OSCAR_snk.loc[(ELUC_OSCAR_snk.index>=time_sta) & (ELUC_OSCAR_snk.index<=time_end)]\n",
    "ELUC_OSCAR_src = ELUC_OSCAR_src.loc[(ELUC_OSCAR_src.index>=time_sta) & (ELUC_OSCAR_src.index<=time_end)]\n",
    "\n",
    "#Check time selection\n",
    "if (ELUC_OSCAR_snk.index[0]!=time_sta) | (ELUC_OSCAR_snk.index[-1]!=time_end) | (len(ELUC_OSCAR_snk.index)!=time_len):  sys.exit('Check time selection of H&N2021!')\n",
    "if (ELUC_OSCAR_src.index[0]!=time_sta) | (ELUC_OSCAR_src.index[-1]!=time_end) | (len(ELUC_OSCAR_src.index)!=time_len):  sys.exit('Check time selection of H&N2021!')\n",
    "\n",
    "# UNITS: Tg C / year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select time\n",
    "time_sta = '2000'\n",
    "time_end = '2020'\n",
    "\n",
    "#Read ISO codes for countries, IPCC countries, and conversions between ISO alpha-3 codes from IPCC and ISO numeric\n",
    "fname_ctrs_ISO   = dir_ctrs + 'wrld_cntrs_BLUE_TN_upd.nc'\n",
    "fname_IPCC_codes = dir_ctrs + 'IPCC_regions.xlsx'\n",
    "fname_ISO_num    = dir_ctrs + 'iso_codes_alpha_numeric.xlsx'\n",
    "fname_cntrs_3LET = dir_ctrs + 'Country codes 3 letters.xlsx'\n",
    "data_IPCC_codes = pd.read_excel(fname_IPCC_codes, sheet_name='region_classification', header=0, usecols=[0, 1, 3])\n",
    "data_alph_num   = pd.read_excel(fname_ISO_num, header=0)\n",
    "data_cntrs_3LET = pd.read_excel(fname_cntrs_3LET, sheet_name=0, header=None, index_col=0)\n",
    "\n",
    "#Read OSCAR regions and countries\n",
    "OSCAR_regions   = pd.read_excel(dir_ELUC_NGHGI + 'OSCAR_all_regions.xlsx', header=0, index_col=0)\n",
    "OSCAR_countries = pd.read_csv(dir_ELUC_NGHGI + 'OSCAR_regions.csv')\n",
    "OSCAR_countries = OSCAR_countries[['Country', 'ISO-Alpha3', 'Finest']]\n",
    "\n",
    "#Regrid country ISO code to model grid\n",
    "file_grid      = dir_grids + 'grid_xy_BLUE'\n",
    "fname_ctrs_reg = dir_tmp + 'wrld_cntrs_BLUE_TN_upd_on_BLUE_grid.nc'\n",
    "if os.path.exists(fname_ctrs_reg): os.remove(fname_ctrs_reg)\n",
    "os.system('cdo remaplaf,' + file_grid + ' ' + fname_ctrs_ISO + ' ' + fname_ctrs_reg)\n",
    "\n",
    "#Read regridded country ISO data\n",
    "data_ctrs_ISO = xr.open_dataset(fname_ctrs_reg)\n",
    "\n",
    "#Read BLUE ELUC on grid (sources and sinks)\n",
    "fname_BLUE_snk_grid = dir_ELUC_2021 + 'ELUC_BLUE_GCB2021_ELUC-sinks-density_' + time_sta + '-' + time_end + '.nc'\n",
    "fname_BLUE_src_grid = dir_ELUC_2021 + 'ELUC_BLUE_GCB2021_ELUC-sources-density_' + time_sta + '-' + time_end + '.nc'\n",
    "data_BLUE_snk_grid = xr.open_dataset(fname_BLUE_snk_grid)\n",
    "data_BLUE_src_grid = xr.open_dataset(fname_BLUE_src_grid)\n",
    "\n",
    "#Read area of BLUE grid\n",
    "fname_area = dir_ELUC + '202109_GCB2021_run_v1_1700-2021__CurrentCPools_CD.nc'\n",
    "data_area = xr.open_dataset(fname_area)\n",
    "data_area = data_area.cell_area\n",
    "\n",
    "#Get ISO values of all countries\n",
    "ISO_values = np.unique(data_ctrs_ISO.ISOcode.values)\n",
    "ISO_values = ISO_values[~np.isnan(ISO_values)]\n",
    "\n",
    "#Define regions\n",
    "regions = ELUC_OSCAR_snk.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = ''\n",
    "\n",
    "#Dataframe for checking values\n",
    "ctr_check = pd.DataFrame(index=regions, columns=['test_perc', 'test_abs'])\n",
    "\n",
    "#Loop over all regions\n",
    "create1 = 1\n",
    "create3 = 1\n",
    "for i, region in enumerate(regions):\n",
    "    \n",
    "    if np.mod(i,10)==0:\n",
    "        print('Run ' + str(i+1) + ' of ' + str(len(regions)))\n",
    "    \n",
    "    #Get country information from OSCAR\n",
    "    ctr_sel = (OSCAR_regions['name']==region)\n",
    "    ctr_sel = OSCAR_regions[ctr_sel]\n",
    "    if ctr_sel.name.values[0]=='Unknown':  continue\n",
    "\n",
    "    #Read OSCAR region data\n",
    "    sel_OSCAR_snk = ELUC_OSCAR_snk[ctr_sel.name]\n",
    "    sel_OSCAR_src = ELUC_OSCAR_src[ctr_sel.name]\n",
    "    \n",
    "    #Get countries in region\n",
    "    countries = OSCAR_countries.loc[OSCAR_countries['Finest']==ctr_sel['number'].values[0]]\n",
    "    \n",
    "    #Arrays for collecting BLUE data\n",
    "    BLUE_snk_sum = pd.DataFrame(columns=countries['ISO-Alpha3'].values)\n",
    "    BLUE_src_sum = pd.DataFrame(columns=countries['ISO-Alpha3'].values)\n",
    "    \n",
    "    #Loop over countries in region\n",
    "    create2 = 1\n",
    "    for index, country in countries.iterrows():\n",
    "        \n",
    "        ctrISO_name = country['ISO-Alpha3']\n",
    "        ctrISO_num  = data_alph_num[data_alph_num['Alpha-3 code']==ctrISO_name]['Numeric'].values[0]\n",
    "        \n",
    "        #Read BLUE country data\n",
    "        sel_BLUE_snk = ELUC_BLUE_snk[ctrISO_name].copy(deep=True)\n",
    "        sel_BLUE_src = ELUC_BLUE_src[ctrISO_name].copy(deep=True)\n",
    "    \n",
    "        #Collect in array\n",
    "        BLUE_snk_sum[ctrISO_name] = sel_BLUE_snk\n",
    "        BLUE_src_sum[ctrISO_name] = sel_BLUE_src\n",
    "        data_ctr_sel = 1 * (data_ctrs_ISO.ISOcode==ctrISO_num)\n",
    "        \n",
    "        #Create region mask\n",
    "        if create2==1:\n",
    "            data_mask = data_ctr_sel\n",
    "            create2 = 0\n",
    "        else:\n",
    "            data_mask = data_mask + data_ctr_sel\n",
    "        \n",
    "        #Collect country selection for overall check\n",
    "        if create3==1:\n",
    "            data_mask_all = data_ctr_sel\n",
    "            create3 = 0\n",
    "        else:\n",
    "            data_mask_all = data_mask_all + data_ctr_sel\n",
    "            \n",
    "    #Sum over all countries in region\n",
    "    BLUE_snk_sum = BLUE_snk_sum.sum(axis=1)\n",
    "    BLUE_src_sum = BLUE_src_sum.sum(axis=1)\n",
    "\n",
    "    #Calculate scale factor\n",
    "    scale_fac_snk = sel_OSCAR_snk[region] / BLUE_snk_sum\n",
    "    scale_fac_src = sel_OSCAR_src[region] / BLUE_src_sum\n",
    "    scale_fac_snk[np.isinf(scale_fac_snk)] = np.NaN\n",
    "    scale_fac_src[np.isinf(scale_fac_src)] = np.NaN\n",
    "    \n",
    "    #Rename scale factor and set NaNs to 0\n",
    "    scale_fac_snk = scale_fac_snk.to_frame(name='scale_factor')\n",
    "    scale_fac_src = scale_fac_src.to_frame(name='scale_factor')\n",
    "    scale_fac_snk[np.isnan(scale_fac_snk)] = 0\n",
    "    scale_fac_src[np.isnan(scale_fac_src)] = 0\n",
    "\n",
    "    #convert scale factor to xarray\n",
    "    scale_snk_xr = scale_fac_snk.to_xarray()\n",
    "    scale_src_xr = scale_fac_src.to_xarray()\n",
    "    scale_snk_xr = scale_snk_xr.rename({'index': 'time'})\n",
    "    scale_src_xr = scale_src_xr.rename({'index': 'time'})\n",
    "\n",
    "    #Get gridded data in selected country\n",
    "    sel_BLUE_grid_snk = data_BLUE_snk_grid.where(data_mask, 0)\n",
    "    sel_BLUE_grid_src = data_BLUE_src_grid.where(data_mask, 0)\n",
    "    \n",
    "    #Apply scale factor\n",
    "    if version=='_vMean':\n",
    "        sel_BLUE_grid_snk = sel_BLUE_grid_snk.sel(time=slice('2001', '2015'))\n",
    "        sel_BLUE_grid_src = sel_BLUE_grid_src.sel(time=slice('2001', '2015'))\n",
    "        scale_snk_xr      = scale_snk_xr.sel(time=slice('2001', '2015'))\n",
    "        scale_src_xr      = scale_src_xr.sel(time=slice('2001', '2015'))\n",
    "        data_OSCAR_snk_scaled = sel_BLUE_grid_snk.mean('time') * scale_snk_xr.scale_factor.mean('time')\n",
    "        data_OSCAR_src_scaled = sel_BLUE_grid_src.mean('time') * scale_src_xr.scale_factor.mean('time')\n",
    "    else:\n",
    "        data_OSCAR_snk_scaled = sel_BLUE_grid_snk * scale_snk_xr.scale_factor\n",
    "        data_OSCAR_src_scaled = sel_BLUE_grid_src * scale_src_xr.scale_factor\n",
    "        \n",
    "    #Distribute OSCAR ELUC sinks equally in country if BLUE ELUC is 0\n",
    "    if np.any(BLUE_snk_sum==0):\n",
    "        \n",
    "        #Get countr area, calculate sink density in country and sSelect times when BLUE is 0\n",
    "        area_ctr = (data_area * data_mask).sum(('lat', 'lon')).values.item()\n",
    "        density_snk = sel_OSCAR_snk[region] / area_ctr * 1e6 # unit: tC / ha\n",
    "        density_snk[np.isnan(density_snk)] = 0\n",
    "        density_snk_xr = density_snk.to_xarray().rename({'index': 'time'})\n",
    "        density_snk_grid = density_snk_xr * data_mask\n",
    "        density_snk_grid = density_snk_grid.to_dataset(name='ELUC')\n",
    "        select_time_snk = (BLUE_snk_sum!=0).to_xarray().to_dataset(name='ELUC')\n",
    "        select_time_snk = select_time_snk.rename({[name for name in select_time_snk.coords][0]:'time'})\n",
    "\n",
    "        #Add calculated density to grid\n",
    "        data_OSCAR_snk_scaled = data_OSCAR_snk_scaled.where(select_time_snk, density_snk_grid)\n",
    "\n",
    "    #Distribute OSCAR ELUC sources equally in country if BLUE ELUC is 0\n",
    "    if np.any(BLUE_src_sum==0):\n",
    "        \n",
    "        #Get countr area, calculate sink density in country and sSelect times when BLUE is 0\n",
    "        area_ctr = (data_area * data_mask).sum(('lat', 'lon')).values.item()\n",
    "        density_src = sel_OSCAR_src[region] / area_ctr * 1e6 # unit: tC / ha\n",
    "        density_src[np.isnan(density_src)] = 0\n",
    "        density_src_xr = density_src.to_xarray().rename({'index': 'time'})\n",
    "        density_src_grid = density_src_xr * data_mask\n",
    "        density_src_grid = density_src_grid.to_dataset(name='ELUC')\n",
    "        select_time_src = (BLUE_src_sum!=0).to_xarray().to_dataset(name='ELUC')\n",
    "        select_time_src = select_time_src.rename({[name for name in select_time_src.coords][0]:'time'})\n",
    "        \n",
    "        #Add calculated density to grid\n",
    "        data_OSCAR_src_scaled = data_OSCAR_src_scaled.where(select_time_src, density_src_grid)        \n",
    "\n",
    "    #Collect in dataset\n",
    "    if create1==1:\n",
    "        data_OSCAR_grid_snk = data_OSCAR_snk_scaled\n",
    "        data_OSCAR_grid_src = data_OSCAR_src_scaled\n",
    "        create1 = 0\n",
    "    else:\n",
    "        data_OSCAR_grid_snk = data_OSCAR_grid_snk + data_OSCAR_snk_scaled\n",
    "        data_OSCAR_grid_src = data_OSCAR_grid_src + data_OSCAR_src_scaled\n",
    "\n",
    "#Define output file name\n",
    "fname_out_snk = dir_ELUC_2021 + 'ELUC_OSCAR_GCB2021_ELUC-sinks-density_' + str(time_sta) + '-' + str(time_end) + version + '.nc'\n",
    "fname_out_src = dir_ELUC_2021 + 'ELUC_OSCAR_GCB2021_ELUC-sources-density_' + str(time_sta) + '-' + str(time_end) + version + '.nc'\n",
    "if os.path.exists(fname_out_snk): os.remove(fname_out_snk)\n",
    "if os.path.exists(fname_out_src): os.remove(fname_out_src)\n",
    "data_OSCAR_grid_snk.to_netcdf(fname_out_snk)\n",
    "data_OSCAR_grid_src.to_netcdf(fname_out_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Clemens Python3.10",
   "language": "python",
   "name": "my-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
