{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_SLAND  = '/Data/SLAND_Trendy-v10_S2_LatLon/'\n",
    "dir_ctrs   = '/Data/data_ancillary/info_countries/'\n",
    "dir_grids  = '/Data/grids/'\n",
    "dir_forest = '/Data/forest_masks/'\n",
    "dir_LSM    = '/Data/LSMs_Trendy-v8/'\n",
    "dir_tmp    = '/Data/tmp/'\n",
    "dir_out    = '/Data/SLAND_Trendy-v10_S2_countries/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate SLAND in forests (using different definitions of forests, and different thresholds for forest cover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define models\n",
    "models = ['CABLE-POP', 'CLASSIC', 'CLM5.0', 'DLEM', 'IBIS', 'ISAM', 'ISBA-CTRIP', 'JSBACH', 'JULES-ES-1.1',\n",
    "          'LPJ-GUESS', 'LPJwsl', 'LPX-Bern', 'OCN', 'ORCHIDEEv3', 'SDGVM', 'VISIT', 'YIBs']#'CLASSIC-N', 'ORCHIDEE'\n",
    "\n",
    "#Define data sets that should be used\n",
    "data_sets = ['DGVMs-Forests-PFTs']#'DGVMs-NaturalLand-PFTs']#, \n",
    "\n",
    "#Select year for forst mask (2000, 2013, or 2016)\n",
    "year = '2013'\n",
    "\n",
    "#Select all forest or non-intact forest\n",
    "selections = ['non-intact-forests']#, 'intact-forests', 'all-forests']\n",
    "\n",
    "#Read ISO codes for countries and conversions between ISO alpha-3 codes from IPCC and ISO numeric\n",
    "fname_ISO_num    = dir_ctrs + 'iso_codes_alpha_numeric.xlsx'\n",
    "fname_IPCC_codes = dir_ctrs + 'IPCC_regions.xlsx'\n",
    "data_IPCC_codes = pd.read_excel(fname_IPCC_codes, sheet_name='region_classification', header=0, usecols=[0, 1, 3])\n",
    "data_alph_num   = pd.read_excel(fname_ISO_num, header=0)\n",
    "\n",
    "#Read ISO codes for countries, IPCC countries, and conversions between ISO alpha-3 codes from IPCC and ISO numeric\n",
    "fname_ctrs_ISO = dir_ctrs + 'wrld_cntrs_BLUE_TN_upd.nc'\n",
    "data_ctrs_ISO  = xr.open_dataset(fname_ctrs_ISO)\n",
    "\n",
    "#Read forest mask\n",
    "fname_mask   = dir_forest + 'Hansen2010_IFL_' + year + '.nc'\n",
    "mask_Potapov = xr.open_dataset(fname_mask)\n",
    "\n",
    "#Read forest fraction in 2013\n",
    "fname_Hansen = dir_forest + 'ForestFraction_0.5deg_2013_regridded-ForestMask.nc'\n",
    "data_Hansen  = xr.open_dataset(fname_Hansen)\n",
    "\n",
    "#Re-index data\n",
    "check_lat1 = np.max(np.abs(data_Hansen['lat'].values - mask_Potapov.lat.values))\n",
    "check_lon1 = np.max(np.abs(data_Hansen['lon'].values - mask_Potapov.lon.values))\n",
    "if check_lat1>0.001 or check_lon1>0.001:  sys.exit('Coordinates do not agree')\n",
    "data_Hansen = data_Hansen.reindex({'lat': mask_Potapov['lat'], 'lon': mask_Potapov['lon']}, method='nearest')\n",
    "\n",
    "#Create NetCDF of global land area\n",
    "fname_landarea_ISO = dir_ctrs + 'wrld_land-area_BLUE_TN_upd.nc'\n",
    "if os.path.exists(fname_landarea_ISO): os.remove(fname_landarea_ISO)\n",
    "data_landarea_ISO = 1 * (data_ctrs_ISO.ISOcode>0)\n",
    "data_landarea_ISO = data_landarea_ISO.to_dataset(name='land_fraction')\n",
    "data_landarea_ISO.to_netcdf(fname_landarea_ISO)        \n",
    "\n",
    "#Loop over datasets\n",
    "for data_set in data_sets:\n",
    "    \n",
    "    #Define different forest cover thresholds according to Hansen et al. (2013)\n",
    "    if data_set=='DGVMs-Forests-PFTs':         Hansen_treshs = [0.30]#[0.05, 0.10, 0.15, 0.20, 0.25]\n",
    "    elif data_set=='DGVMs-NaturalLand-PFTs':   Hansen_treshs = [0.15]    \n",
    "\n",
    "    #Select maximum weighting factor of 50\n",
    "    if 'NaturalLand' in data_set:\n",
    "        for_varname = 'w_NLCF'\n",
    "    else:\n",
    "        for_varname = 'w_FF'\n",
    "    \n",
    "    #Loop over different forest cover thresholds according to Hansen et al. (2013)\n",
    "    for Han2013_thresh in Hansen_treshs:\n",
    "    \n",
    "        #Loop over different forest definitions\n",
    "        for selection in selections:\n",
    "\n",
    "            #Select which forests to include (see Grassi et al. (2021), who used 20% threshold for forest cover)\n",
    "            if selection=='non-intact-forests':\n",
    "                mask_forest = data_Hansen.forest_fraction.where(mask_Potapov.Band1!=2)\n",
    "                mask_forest = mask_forest > 0.0\n",
    "            elif selection=='intact-forests':\n",
    "                mask_forest = data_Hansen.forest_fraction.where(mask_Potapov.Band1==2)\n",
    "                mask_forest = mask_forest > 0.0\n",
    "            elif selection=='all-forests':\n",
    "                mask_forest = data_Hansen.forest_fraction > 0.0\n",
    "\n",
    "            #Create dicts for storing data\n",
    "            SLAND_coll = dict()\n",
    "\n",
    "            #Loop over models\n",
    "            for model in models:\n",
    "\n",
    "                print(model)\n",
    "\n",
    "                #Conservatively regrid global land area to DGVM grid\n",
    "                fname_landarea_ISO_regr = dir_tmp + 'land-area_cntrs_BLUE_TN_upd_regrid_' + model + '_for_tmp.nc'\n",
    "                if os.path.exists(fname_landarea_ISO_regr): os.remove(fname_landarea_ISO_regr)\n",
    "                file_grid = dir_grids + 'grid_xy_' + model\n",
    "                os.system('cdo remapcon,' + file_grid + ' ' + fname_landarea_ISO + ' ' + fname_landarea_ISO_regr)\n",
    "\n",
    "                #Read regridded land area file \n",
    "                data_landarea_ISO = xr.open_dataset(fname_landarea_ISO_regr)\n",
    "\n",
    "                #Get file name for SLAND\n",
    "                fnames = [file for file in os.listdir(dir_SLAND) if (model + '_' in file) and ('SLAND_NBP.nc' in file) and ('NBPPFT' not in file)]\n",
    "                if len(fnames)!=1:  sys.exit('Filename not unique')\n",
    "\n",
    "                #Read SLAND data\n",
    "                fname = dir_SLAND + fnames[0]\n",
    "                data_SLAND = xr.open_dataset(fname)\n",
    "\n",
    "                #Define file name for weighting factors\n",
    "                if data_set=='DGVMs-Forests-PFTs':        fname_weight = dir_forest + 'Weights_ForestFraction_DGVMs-S2-S3_grid_' + model + '.nc'\n",
    "                elif data_set=='DGVMs-NaturalLand-PFTs':  fname_weight = dir_forest + 'Weights_NaturalLandCoverFraction_DGVMs-S2-S3_grid_' + model + '.nc'\n",
    "\n",
    "                #Distinguish whethera weighing mask exists or not for each model\n",
    "                if os.path.exists(fname_weight):\n",
    "\n",
    "                    #Flag for deleting files\n",
    "                    del_files = 1\n",
    "\n",
    "                    #Define file names for remapping (in case different Trendy versions use different grids)\n",
    "                    fname_weight_tmp    = dir_tmp + 'weights_' + model + '_for_tmp.nc'\n",
    "                    fname_weight_regrid = dir_tmp + 'weights_' + model + '_regridded_for_tmp.nc'\n",
    "                    if os.path.exists(fname_weight_tmp):     os.remove(fname_weight_tmp)\n",
    "                    if os.path.exists(fname_weight_regrid):  os.remove(fname_weight_regrid)\n",
    "\n",
    "                    #Create temporary reference grid and set grid of weighting factor\n",
    "                    fname_grid_tmp = dir_tmp + 'grid_xy_for_' + model \n",
    "                    if os.path.exists(fname_grid_tmp): os.remove(fname_grid_tmp)\n",
    "                    os.system('cdo -s griddes -selvar,SLAND ' + fname + ' > ' + fname_grid_tmp)\n",
    "                    os.system('cdo setgrid,' + fname_grid_tmp + ' ' + fname_weight + ' ' + fname_weight_tmp)\n",
    "\n",
    "                    #Remap weighting factors to DGVM grid \n",
    "                    os.system('cdo remapcon,' + file_grid + ' ' + fname_weight_tmp + ' ' + fname_weight_regrid)\n",
    "\n",
    "                    #Read regridded weighting factors\n",
    "                    for_weight = xr.open_dataset(fname_weight_regrid)\n",
    "                    weight     = for_weight[for_varname]\n",
    "\n",
    "                else:\n",
    "\n",
    "                    #Flag for deleting files\n",
    "                    del_files = 2\n",
    "\n",
    "                    #Read weighting factors of all other DGVMs and calculate multi-model mean\n",
    "                    if data_set=='DGVMs-Forests-PFTs':        fname_weight2 = dir_forest + 'Weights_ForestFraction_DGVMs-S2-S3_regrid360x720-ForestMask.nc'  \n",
    "                    elif data_set=='DGVMs-NaturalLand-PFTs':  fname_weight2 = dir_forest + 'Weights_NaturalLandCoverFraction_DGVMs-S2-S3_regrid360x720-ForestMask.nc'\n",
    "                    for_weight = xr.open_dataset(fname_weight2)\n",
    "                    for_weight = for_weight.mean('model')\n",
    "\n",
    "                    #Define file names for remapping\n",
    "                    fname_weight_tmp1   = dir_tmp + 'weights_' + model + '_for_tmp1.nc'\n",
    "                    fname_weight_tmp2   = dir_tmp + 'weights_' + model + '_for_tmp2.nc'\n",
    "                    fname_weight_regrid = dir_tmp + 'weights_' + model + '_regridded_for_tmp.nc'\n",
    "                    if os.path.exists(fname_weight_tmp1):     os.remove(fname_weight_tmp1)\n",
    "                    if os.path.exists(fname_weight_tmp2):     os.remove(fname_weight_tmp2)\n",
    "                    if os.path.exists(fname_weight_regrid):  os.remove(fname_weight_regrid)\n",
    "\n",
    "                    #Save multi-model mean as NetCDF and set correct grid\n",
    "                    for_weight.to_netcdf(fname_weight_tmp1)\n",
    "                    file_grid_FM = '/work/mj0060/m300896/Trendy/Data/grids/grid_xy_360x720'\n",
    "                    os.system('cdo setgrid,' + file_grid_FM + ' ' + fname_weight_tmp1 + ' ' + fname_weight_tmp2)\n",
    "\n",
    "                    #Remap weighting factors to model grid\n",
    "                    os.system('cdo remapcon,' + file_grid + ' ' + fname_weight_tmp2 + ' ' + fname_weight_regrid)\n",
    "\n",
    "                    #Read regridded weighting factors\n",
    "                    for_weight = xr.open_dataset(fname_weight_regrid)\n",
    "                    weight     = for_weight[for_varname]\n",
    "\n",
    "                #Get lat and lon names\n",
    "                if 'latitude' in data_SLAND.dims:  lat_name, lon_name = 'latitude', 'longitude'\n",
    "                else:                              lat_name, lon_name = 'lat', 'lon'\n",
    "\n",
    "                #Re-index data if necessary\n",
    "                check_lat1 = np.max(np.abs(weight[lat_name].values - data_SLAND[lat_name].values))\n",
    "                check_lon1 = np.max(np.abs(weight[lon_name].values - data_SLAND[lon_name].values))\n",
    "                check_lat2 = np.max(np.abs(weight[lat_name].values - data_landarea_ISO[lat_name].values))\n",
    "                check_lon2 = np.max(np.abs(weight[lon_name].values - data_landarea_ISO[lon_name].values))\n",
    "                if check_lat1>0.001 or check_lon1>0.001:  sys.exit('Coordinates do not agree')\n",
    "                if check_lat2>0.001 or check_lon2>0.001:  sys.exit('Coordinates do not agree')\n",
    "                weight     = weight.reindex({lat_name: data_landarea_ISO[lat_name], lon_name: data_landarea_ISO[lon_name]}, method='nearest')\n",
    "                data_SLAND = data_SLAND.reindex({lat_name: data_landarea_ISO[lat_name], lon_name: data_landarea_ISO[lon_name]}, method='nearest')\n",
    "\n",
    "                #Define temporary file name for regridding forest mask\n",
    "                fname_for_tmp        = dir_tmp + 'for_selection_' + model + '_for_tmp.nc'\n",
    "                fname_for_tmp_regrid = dir_tmp + 'for_selection_' + model + '_regridded_for_tmp.nc'\n",
    "                if os.path.exists(fname_for_tmp):         os.remove(fname_for_tmp)\n",
    "                if os.path.exists(fname_for_tmp_regrid):  os.remove(fname_for_tmp_regrid)                    \n",
    "\n",
    "                #Calculate forest weights and save it in NetCDF\n",
    "                forest_weight = 1 * (data_Hansen.forest_fraction>Han2013_thresh) & (mask_forest==1)\n",
    "\n",
    "                #Save forest weights it in NetCDF\n",
    "                forest_weight.to_dataset(name='forest').to_netcdf(fname_for_tmp)\n",
    "\n",
    "                #Remap forest weights to DGVM grid and read regridded data\n",
    "                os.system('cdo remapcon,' + file_grid + ' ' + fname_for_tmp + ' ' + fname_for_tmp_regrid)\n",
    "                forest_weight_regr = xr.open_dataset(fname_for_tmp_regrid)\n",
    "\n",
    "                #Define weight for all land and apply weighting\n",
    "                weight = weight.where(weight>0, 1)\n",
    "                data_SLAND = data_SLAND * weight * forest_weight_regr['forest']\n",
    "\n",
    "                #Create dict for storing data\n",
    "                SLAND_ctrs = dict()\n",
    "                \n",
    "                #Loop over all country codes\n",
    "                for i, iso_alpha3 in enumerate(data_IPCC_codes['ISO']):\n",
    "\n",
    "                    if np.mod(i, 50)==0:\n",
    "                        print('  -run ' + str(i+1) + ' of ' + str(len(data_IPCC_codes['ISO'])))\n",
    "\n",
    "                    #Get numbeic ISO code of country\n",
    "                    iso_numeric = data_alph_num['Numeric'][data_alph_num['Alpha-3 code']==iso_alpha3].values[0]\n",
    "\n",
    "                    #Select country in country mask\n",
    "                    mask_ISO =  1 * (data_ctrs_ISO.ISOcode==iso_numeric)\n",
    "\n",
    "                    #Define temporary file names for selecting and regridding country\n",
    "                    fname_tmp      = dir_tmp + 'country_fraction_' + iso_alpha3 + '_' + model + '_for_tmp.nc'\n",
    "                    fname_tmp_regr = dir_tmp + 'country_fraction_' + iso_alpha3 + '_' + model + '_regr_for_tmp.nc'\n",
    "                    if os.path.exists(fname_tmp):       os.remove(fname_tmp)\n",
    "                    if os.path.exists(fname_tmp_regr):  os.remove(fname_tmp_regr)\n",
    "\n",
    "                    #Save country fraction file in NetCDF\n",
    "                    mask_ISO = mask_ISO.to_dataset(name='country_fraction')\n",
    "                    mask_ISO.to_netcdf(fname_tmp)\n",
    "\n",
    "                    #Conservatively regrid country fraction to DGVM grid\n",
    "                    os.system('cdo -s remapcon,' + file_grid + ' ' + fname_tmp + ' ' + fname_tmp_regr)\n",
    "\n",
    "                    #Read regridded country fraction file\n",
    "                    mask_ISO_regr = xr.open_dataset(fname_tmp_regr)\n",
    "\n",
    "                    #Perform calculation (if country fraction is not 0 everywhere)\n",
    "                    if len(mask_ISO_regr.data_vars)!=0:\n",
    "\n",
    "                        #Create weighting factor (= fraction of country / land fraction)\n",
    "                        weights = mask_ISO_regr.country_fraction / data_landarea_ISO.land_fraction\n",
    "\n",
    "                        #Get SLAND sum in selected country\n",
    "                        data_sel = (data_SLAND * weights).sum((lat_name, lon_name))\n",
    "\n",
    "                        #Convert to Tg C/year\n",
    "                        data_sel = 1000 * data_sel\n",
    "\n",
    "                        #Save values in dict\n",
    "                        SLAND_ctrs[iso_alpha3] = data_sel.SLAND.values\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        #Set SLAND to 0 if country is too small\n",
    "                        SLAND_ctrs[iso_alpha3] = np.zeros(len(data_SLAND.time))\n",
    "\n",
    "                    #Remove temporary files\n",
    "                    os.remove(fname_tmp)\n",
    "                    os.remove(fname_tmp_regr)\n",
    "\n",
    "                #Convert data to data frame (and sort by country name)\n",
    "                SLAND_ctrs_df = pd.DataFrame(SLAND_ctrs, index=data_SLAND.time.dt.year)\n",
    "                SLAND_ctrs_df = SLAND_ctrs_df.reindex(sorted(SLAND_ctrs_df.columns), axis=1)\n",
    "\n",
    "                #Adde units in first cell\n",
    "                SLAND_ctrs_df = SLAND_ctrs_df.rename_axis('unit: TG C/year')\n",
    "    \n",
    "                #Save data in dictionary\n",
    "                SLAND_coll[model] = SLAND_ctrs_df\n",
    "\n",
    "                #Remove temporary files\n",
    "                os.remove(fname_landarea_ISO_regr)\n",
    "                os.remove(fname_weight_regrid)\n",
    "                os.remove(fname_for_tmp)\n",
    "                os.remove(fname_for_tmp_regrid)\n",
    "                if del_files==1:\n",
    "                    os.remove(fname_grid_tmp)\n",
    "                    os.remove(fname_weight_tmp)\n",
    "                elif del_files==2:\n",
    "                    os.remove(fname_weight_tmp1)\n",
    "                    os.remove(fname_weight_tmp2)\n",
    "\n",
    "            #Define output file name\n",
    "            fname_out = dir_out + 'SLAND-S2-countries_' + selection + '_Weights-' + data_set + '_MaxForCover' + '{:.2f}'.format(Han2013_thresh) + '_vRemapToForestMask-Hansen2010_IFL_' + year + '_v2.xlsx'\n",
    "            if os.path.exists(fname_out): os.remove(fname_out)\n",
    "\n",
    "            #Create xlsx-file\n",
    "            with pd.ExcelWriter(fname_out) as writer:\n",
    "\n",
    "                #Loop over models\n",
    "                for model in models:\n",
    "                    \n",
    "                    #Save as sheet in excel\n",
    "                    SLAND_coll[model].to_excel(writer, sheet_name=model + '_SLAND_IPCC_ctrs', index=True, header=True, float_format='%.6f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate total weighted SLAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define models\n",
    "models = ['CABLE-POP', 'CLASSIC', 'CLM5.0', 'DLEM', 'IBIS', 'ISAM', 'ISBA-CTRIP', 'JSBACH', 'JULES-ES-1.1',\n",
    "          'LPJ-GUESS', 'LPJwsl', 'LPX-Bern', 'OCN', 'ORCHIDEEv3', 'SDGVM', 'VISIT', 'YIBs']#'CLASSIC-N', 'ORCHIDEE'\n",
    "\n",
    "#Select year for forst mask (2000, 2013, or 2016)\n",
    "year = '2013'\n",
    "\n",
    "#Read ISO codes for countries and conversions between ISO alpha-3 codes from IPCC and ISO numeric\n",
    "fname_ISO_num    = dir_ctrs + 'iso_codes_alpha_numeric.xlsx'\n",
    "fname_IPCC_codes = dir_ctrs + 'IPCC_regions.xlsx'\n",
    "data_IPCC_codes = pd.read_excel(fname_IPCC_codes, sheet_name='region_classification', header=0, usecols=[0, 1, 3])\n",
    "data_alph_num   = pd.read_excel(fname_ISO_num, header=0)\n",
    "\n",
    "#Read ISO codes for countries, IPCC countries, and conversions between ISO alpha-3 codes from IPCC and ISO numeric\n",
    "fname_ctrs_ISO = dir_ctrs + 'wrld_cntrs_BLUE_TN_upd.nc'\n",
    "data_ctrs_ISO  = xr.open_dataset(fname_ctrs_ISO)\n",
    "\n",
    "#Create NetCDF of global land area\n",
    "fname_landarea_ISO = dir_ctrs + 'wrld_land-area_BLUE_TN_upd.nc'\n",
    "if os.path.exists(fname_landarea_ISO): os.remove(fname_landarea_ISO)\n",
    "data_landarea_ISO = 1 * (data_ctrs_ISO.ISOcode>0)\n",
    "data_landarea_ISO = data_landarea_ISO.to_dataset(name='land_fraction')\n",
    "data_landarea_ISO.to_netcdf(fname_landarea_ISO)\n",
    "\n",
    "#Create dicts for storing data\n",
    "SLAND_coll = dict()\n",
    "\n",
    "#Loop over models\n",
    "for model in models:\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    #Conservatively regrid global land area to DGVM grid\n",
    "    fname_landarea_ISO_regr = dir_tmp + 'land-area_cntrs_BLUE_TN_upd_regrid_' + model + '_tmp.nc'\n",
    "    if os.path.exists(fname_landarea_ISO_regr): os.remove(fname_landarea_ISO_regr)\n",
    "    file_grid = dir_grids + 'grid_xy_' + model\n",
    "    os.system('cdo remapcon,' + file_grid + ' ' + fname_landarea_ISO + ' ' + fname_landarea_ISO_regr)\n",
    "\n",
    "    #Read regridded land area file \n",
    "    data_landarea_ISO = xr.open_dataset(fname_landarea_ISO_regr)\n",
    "\n",
    "    #Get file name for SLAND\n",
    "    fnames = [file for file in os.listdir(dir_SLAND) if (model + '_' in file) and ('SLAND_NBP.nc' in file) and ('NBPPFT' not in file)]\n",
    "    if len(fnames)!=1:  sys.exit('Filename not unique')\n",
    "\n",
    "    #Read SLAND data\n",
    "    fname = dir_SLAND + fnames[0]\n",
    "    data_SLAND = xr.open_dataset(fname)\n",
    "\n",
    "    #Define file name for weighting factors for forest\n",
    "    fname_weight = dir_forest + 'Weights_ForestFraction_DGVMs-S2-S3_grid_' + model + '.nc'\n",
    "\n",
    "    #Distinguish whethera weighing mask exists or not for each model\n",
    "    if os.path.exists(fname_weight):\n",
    "\n",
    "        #Flag for deleting files\n",
    "        del_files = 1\n",
    "\n",
    "        #Define file names for remapping (in case different Trendy versions use different grids)\n",
    "        fname_weight_tmp    = dir_tmp + 'weights_' + model + '_tmp.nc'\n",
    "        fname_weight_regrid = dir_tmp + 'weights_' + model + '_regridded_tmp.nc'\n",
    "        if os.path.exists(fname_weight_tmp):     os.remove(fname_weight_tmp)\n",
    "        if os.path.exists(fname_weight_regrid):  os.remove(fname_weight_regrid)\n",
    "\n",
    "        #Create temporary reference grid and set grid of weighting factor\n",
    "        fname_grid_tmp = dir_tmp + 'grid_xy_' + model \n",
    "        if os.path.exists(fname_grid_tmp): os.remove(fname_grid_tmp)\n",
    "        os.system('cdo -s griddes -selvar,SLAND ' + fname + ' > ' + fname_grid_tmp)\n",
    "        os.system('cdo setgrid,' + fname_grid_tmp + ' ' + fname_weight + ' ' + fname_weight_tmp)\n",
    "\n",
    "        #Remap weighting factors to DGVM grid \n",
    "        os.system('cdo remapcon,' + file_grid + ' ' + fname_weight_tmp + ' ' + fname_weight_regrid)\n",
    "\n",
    "        #Read regridded weighting factors\n",
    "        for_weight = xr.open_dataset(fname_weight_regrid)\n",
    "        weight     = for_weight.w_FF\n",
    "\n",
    "    else:\n",
    "\n",
    "        #Flag for deleting files\n",
    "        del_files = 2\n",
    "\n",
    "        #Read weighting factors of all other DGVMs and calculate multi-model mean\n",
    "        fname_weight = dir_forest + 'Weights_ForestFraction_DGVMs-S2-S3_regrid360x720-ForestMask.nc'\n",
    "        for_weight = xr.open_dataset(fname_weight)\n",
    "        for_weight = for_weight.mean('model')\n",
    "\n",
    "        #Define file names for remapping\n",
    "        fname_weight_tmp1   = dir_tmp + 'weights_' + model + '_tmp1.nc'\n",
    "        fname_weight_tmp2   = dir_tmp + 'weights_' + model + '_tmp2.nc'\n",
    "        fname_weight_regrid = dir_tmp + 'weights_' + model + '_regridded_tmp.nc'\n",
    "        if os.path.exists(fname_weight_tmp1):     os.remove(fname_weight_tmp1)\n",
    "        if os.path.exists(fname_weight_tmp2):     os.remove(fname_weight_tmp2)\n",
    "        if os.path.exists(fname_weight_regrid):  os.remove(fname_weight_regrid)\n",
    "\n",
    "        #Save multi-model mean as NetCDF and set correct grid\n",
    "        for_weight.to_netcdf(fname_weight_tmp1)\n",
    "        file_grid_FM = '/work/mj0060/m300896/Trendy/Data/grids/grid_xy_360x720'\n",
    "        os.system('cdo setgrid,' + file_grid_FM + ' ' + fname_weight_tmp1 + ' ' + fname_weight_tmp2)\n",
    "\n",
    "        #Remap weighting factors to model grid\n",
    "        os.system('cdo remapcon,' + file_grid + ' ' + fname_weight_tmp2 + ' ' + fname_weight_regrid)\n",
    "\n",
    "        #Read regridded weighting factors\n",
    "        for_weight = xr.open_dataset(fname_weight_regrid)\n",
    "        weight     = for_weight.w_FF\n",
    "\n",
    "    #Get lat and lon names\n",
    "    if 'latitude' in data_SLAND.dims:  lat_name, lon_name = 'latitude', 'longitude'\n",
    "    else:                              lat_name, lon_name = 'lat', 'lon'\n",
    "\n",
    "    #Re-index data if necessary\n",
    "    check_lat1 = np.max(np.abs(weight[lat_name].values - data_SLAND[lat_name].values))\n",
    "    check_lon1 = np.max(np.abs(weight[lon_name].values - data_SLAND[lon_name].values))\n",
    "    check_lat2 = np.max(np.abs(weight[lat_name].values - data_landarea_ISO[lat_name].values))\n",
    "    check_lon2 = np.max(np.abs(weight[lon_name].values - data_landarea_ISO[lon_name].values))\n",
    "    if check_lat1>0.001 or check_lon1>0.001:  sys.exit('Coordinates do not agree')\n",
    "    if check_lat2>0.001 or check_lon2>0.001:  sys.exit('Coordinates do not agree')\n",
    "    weight     = weight.reindex({lat_name: data_landarea_ISO[lat_name], lon_name: data_landarea_ISO[lon_name]}, method='nearest')\n",
    "    data_SLAND = data_SLAND.reindex({lat_name: data_landarea_ISO[lat_name], lon_name: data_landarea_ISO[lon_name]}, method='nearest')\n",
    "\n",
    "    #Define weight for all land and apply weighting\n",
    "    weight = weight.where(weight>0, 1)\n",
    "    data_SLAND = data_SLAND * weight\n",
    "    \n",
    "    #Create dicts for storing data\n",
    "    SLAND_ctrs = dict()\n",
    "\n",
    "    #Loop over all country codes\n",
    "    for i, iso_alpha3 in enumerate(data_IPCC_codes['ISO']):\n",
    "\n",
    "        if np.mod(i, 50)==0:\n",
    "            print('  -run ' + str(i+1) + ' of ' + str(len(data_IPCC_codes['ISO'])))\n",
    "\n",
    "        #Get numbeic ISO code of country\n",
    "        iso_numeric = data_alph_num['Numeric'][data_alph_num['Alpha-3 code']==iso_alpha3].values[0]\n",
    "\n",
    "        #Select country in country mask\n",
    "        mask_ISO =  1 * (data_ctrs_ISO.ISOcode==iso_numeric)\n",
    "\n",
    "        #Define temporary file names for selecting and regridding country\n",
    "        fname_tmp      = dir_tmp + 'country_fraction_' + iso_alpha3 + '_' + model + '_tmp.nc'\n",
    "        fname_tmp_regr = dir_tmp + 'country_fraction_' + iso_alpha3 + '_' + model + '_regr_tmp.nc'\n",
    "        if os.path.exists(fname_tmp):       os.remove(fname_tmp)\n",
    "        if os.path.exists(fname_tmp_regr):  os.remove(fname_tmp_regr)\n",
    "\n",
    "        #Save country fraction file in NetCDF\n",
    "        mask_ISO = mask_ISO.to_dataset(name='country_fraction')\n",
    "        mask_ISO.to_netcdf(fname_tmp)\n",
    "\n",
    "        #Conservatively regrid country fraction to DGVM grid\n",
    "        os.system('cdo -s remapcon,' + file_grid + ' ' + fname_tmp + ' ' + fname_tmp_regr)\n",
    "\n",
    "        #Read regridded country fraction file\n",
    "        mask_ISO_regr = xr.open_dataset(fname_tmp_regr)\n",
    "\n",
    "        #Perform calculation (if country fraction is not 0 everywhere)\n",
    "        if len(mask_ISO_regr.data_vars)!=0:\n",
    "\n",
    "            #Create weighting factor (= fraction of country / land fraction)\n",
    "            weights = mask_ISO_regr.country_fraction / data_landarea_ISO.land_fraction\n",
    "\n",
    "            #Get SLAND sum in selected country\n",
    "            data_sel = (data_SLAND * weights).sum((lat_name, lon_name))\n",
    "\n",
    "            #Convert to Tg C/year\n",
    "            data_sel = 1000 * data_sel\n",
    "\n",
    "            #Save values in dict\n",
    "            SLAND_ctrs[iso_alpha3] = data_sel.SLAND.values\n",
    "\n",
    "        else:\n",
    "\n",
    "            #Set SLAND to 0 if country is too small\n",
    "            SLAND_ctrs[iso_alpha3] = np.zeros(len(data_SLAND.time))\n",
    "\n",
    "        #Remove temporary files\n",
    "        os.remove(fname_tmp)\n",
    "        os.remove(fname_tmp_regr)\n",
    "\n",
    "\n",
    "    #Convert data to data frame (and sort by country name)\n",
    "    SLAND_ctrs_df = pd.DataFrame(SLAND_ctrs, index=data_SLAND.time.dt.year)\n",
    "    SLAND_ctrs_df = SLAND_ctrs_df.reindex(sorted(SLAND_ctrs_df.columns), axis=1)\n",
    "\n",
    "    #Add units in first cell\n",
    "    SLAND_ctrs_df = SLAND_ctrs_df.rename_axis('unit: TG C/year')\n",
    "    \n",
    "    #Save data in dictionary\n",
    "    SLAND_coll[model] = SLAND_ctrs_df\n",
    "\n",
    "    #Remove temporary files\n",
    "    os.remove(fname_landarea_ISO_regr)\n",
    "    os.remove(fname_weight_regrid)\n",
    "    if del_files==1:\n",
    "        os.remove(fname_grid_tmp)\n",
    "        os.remove(fname_weight_tmp)\n",
    "    elif del_files==2:\n",
    "        os.remove(fname_weight_tmp1)\n",
    "        os.remove(fname_weight_tmp2)\n",
    "\n",
    "#Define output file name\n",
    "fname_out = dir_out + 'SLAND-S2-countries_Weights-DGVMs-Forests-PFTs_total-SLAND_vRemapToForestMask-Hansen2010_IFL_' + year + '_v2.xlsx'\n",
    "if os.path.exists(fname_out): os.remove(fname_out)\n",
    "\n",
    "#Create xlsx-file\n",
    "with pd.ExcelWriter(fname_out) as writer:        \n",
    "    \n",
    "    #Loop over models\n",
    "    for model in models:\n",
    "        \n",
    "        #Save as sheet in excel\n",
    "        SLAND_coll[model].to_excel(writer, sheet_name=model + '_SLAND_IPCC_ctrs', index=True, header=True, float_format='%.6f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (based on the module python3/2022.01)",
   "language": "python",
   "name": "python3_2022_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
