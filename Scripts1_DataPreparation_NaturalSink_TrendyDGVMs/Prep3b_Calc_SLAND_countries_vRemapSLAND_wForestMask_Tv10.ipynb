{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_SLAND  = '/Trendy/Data/SLAND_Trendy-v10_S2_LatLon/'\n",
    "dir_ctrs   = '/Trendy/Data/data_ancillary/info_countries/'\n",
    "dir_grids  = '/Trendy/Data/grids/'\n",
    "dir_forest = '/GCB2021_commentary/Data/forest_masks/'\n",
    "dir_LSM    = '/Trendy/Data/LSMs_Trendy-v8/'\n",
    "dir_tmp    = '/Trendy/Data/tmp/'\n",
    "dir_out    = '/Trendy/Data/SLAND_Trendy-v10_S2_countries/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regrid countries, land-sea mask, and forest area to forest mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regrid country ISO code to model grid\n",
    "file_grid      = dir_grids + 'grid_xy_360x720-ForestMask'\n",
    "fname_ctrs     = dir_ctrs + 'wrld_cntrs_BLUE_TN_upd.nc'\n",
    "fname_ctrs_reg = dir_tmp + 'wrld_cntrs_BLUE_TN_upd_on_ForestMask_grid.nc'\n",
    "if os.path.exists(fname_ctrs_reg):  os.remove(fname_ctrs_reg)\n",
    "os.system('cdo remaplaf,' + file_grid + ' ' + fname_ctrs + ' ' + fname_ctrs_reg)\n",
    "\n",
    "#Regrid country ISO code to model grid\n",
    "file_grid = dir_grids + 'grid_xy_360x720-ForestMask'\n",
    "fname_in  = dir_LSM + 'LSM_reference.nc'\n",
    "fname_out = dir_LSM + 'LandSeaMask_360x720-ForestMask.nc'\n",
    "if os.path.exists(fname_out):  os.remove(fname_out)\n",
    "os.system('cdo remapcon,' + file_grid + ' ' + fname_in + ' ' + fname_out)\n",
    "\n",
    "#Interpolate forest fraction to grid of forest mask\n",
    "fname_grid          = dir_grids + 'grid_xy_360x720-ForestMask'\n",
    "fname_for_frac      = dir_forest + 'ForestFraction_0.5deg_2013.nc'\n",
    "fname_for_frac_regr = dir_forest + 'ForestFraction_0.5deg_2013_regridded-ForestMask.nc'\n",
    "if os.path.exists(fname_for_frac_regr):  os.remove(fname_for_frac_regr)\n",
    "os.system('cdo remapcon,' + fname_grid + \" \" + fname_for_frac + \" \" + fname_for_frac_regr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate SLAND in forests (using different definitions of forests, and different thresholds for forest cover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define models\n",
    "models = ['CABLE-POP', 'CLASSIC', 'CLASSIC-N', 'CLM5.0', 'DLEM', 'IBIS', 'ISAM', 'ISBA-CTRIP', 'JSBACH', 'JULES-ES-1.1',\n",
    "          'LPJ-GUESS', 'LPJwsl', 'LPX-Bern', 'OCN', 'ORCHIDEE', 'ORCHIDEEv3', 'SDGVM', 'VISIT', 'YIBs']\n",
    "\n",
    "#Define data sets that should be used\n",
    "data_sets = ['DGVMs-NaturalLand-PFTs']#, 'DGVMs-Forests-PFTs', 'BLUE']\n",
    "\n",
    "#Define grid size\n",
    "gridsize = '360x720-ForestMask'\n",
    "\n",
    "#Select year for forst mask (2000, 2013, or 2016)\n",
    "year = '2013'\n",
    "\n",
    "#Select all forest or non-intact forest\n",
    "selections = ['non-intact-forests']#, 'intact-forests', 'all-forests']\n",
    "\n",
    "#Read ISO codes for countries, IPCC countries, and conversions between ISO alpha-3 codes from IPCC and ISO numeric\n",
    "fname_ctrs_ISO   = dir_tmp + 'wrld_cntrs_BLUE_TN_upd_on_ForestMask_grid.nc'\n",
    "fname_IPCC_codes = dir_ctrs + 'IPCC_regions.xlsx'\n",
    "fname_ISO_num    = dir_ctrs + 'iso_codes_alpha_numeric.xlsx'\n",
    "data_ctrs_ISO   = xr.open_dataset(fname_ctrs_ISO)\n",
    "data_IPCC_codes = pd.read_excel(fname_IPCC_codes, sheet_name='region_classification', header=0, usecols=[0, 1, 3])\n",
    "data_alph_num   = pd.read_excel(fname_ISO_num, header=0)\n",
    "\n",
    "#Read forest mask\n",
    "fname_mask   = dir_forest + 'Hansen2010_IFL_' + year + '.nc'\n",
    "mask_Potapov = xr.open_dataset(fname_mask)\n",
    "\n",
    "#Read forest fraction in 2013\n",
    "fname_Hansen = dir_forest + 'ForestFraction_0.5deg_2013_regridded-ForestMask.nc'\n",
    "data_Hansen  = xr.open_dataset(fname_Hansen)\n",
    "\n",
    "for data_set in data_sets:\n",
    "    \n",
    "    #Define different forest cover thresholds according to Hansen et al. (2013)\n",
    "    if data_set=='DGVMs-Forests-PFTs':         Hansen_treshs = [0.05, 0.10, 0.15, 0.20, 0.25]#, 0.30, 0.35]\n",
    "    elif data_set=='DGVMs-NaturalLand-PFTs':   Hansen_treshs = [0.15]    \n",
    "    \n",
    "    #Define file name\n",
    "    if data_set=='BLUE':                      fname_weight = dir_forest + 'Weights_ForestFraction_BLUE_regrid360x720-ForestMask.nc'\n",
    "    elif data_set=='DGVMs-Forests-PFTs':      fname_weight = dir_forest + 'Weights_ForestFraction_DGVMs-S2-S3_regrid360x720-ForestMask.nc'\n",
    "    elif data_set=='DGVMs-NaturalLand-PFTs':  fname_weight = dir_forest + 'Weights_NaturalLandCoverFraction_DGVMs-S2-S3_regrid360x720-ForestMask.nc'\n",
    "\n",
    "    #Read weight factors for forest\n",
    "    for_weight = xr.open_dataset(fname_weight)\n",
    "\n",
    "    #Select maximum weighting factor of 50\n",
    "    if 'NaturalLand' in data_set:\n",
    "        for_weight = for_weight.w_NLCF\n",
    "    else:\n",
    "        for_weight = for_weight.w_FF\n",
    "    \n",
    "    #Re-index data\n",
    "    check_lat1 = np.max(np.abs(data_Hansen['lat'].values - mask_Potapov.lat.values))\n",
    "    check_lon1 = np.max(np.abs(data_Hansen['lon'].values - mask_Potapov.lon.values))\n",
    "    check_lat2 = np.max(np.abs(for_weight['lat'].values - mask_Potapov.lat.values))\n",
    "    check_lon2 = np.max(np.abs(for_weight['lon'].values - mask_Potapov.lon.values))\n",
    "    if check_lat1>0.001 or check_lon1>0.001:  sys.exit('Latitudes do not agree')\n",
    "    if check_lat2>0.001 or check_lon2>0.001:  sys.exit('Latitudes do not agree')\n",
    "    data_Hansen = data_Hansen.reindex({'lat': mask_Potapov['lat'], 'lon': mask_Potapov['lon']}, method='nearest')\n",
    "    for_weight  = for_weight.reindex({'lat': mask_Potapov['lat'], 'lon': mask_Potapov['lon']}, method='nearest')\n",
    "\n",
    "    #Loop over different forest cover thresholds according to Hansen et al. (2013)\n",
    "    for Han2013_thresh in Hansen_treshs:\n",
    "    \n",
    "        #Loop over different forest definitions\n",
    "        for selection in selections:\n",
    "\n",
    "            #Select which forests to include (see Grassi et al. (2021), who used 20% threshold for forest cover)\n",
    "            if selection=='non-intact-forests':\n",
    "                mask_forest = data_Hansen.forest_fraction.where(mask_Potapov.Band1!=2)\n",
    "                mask_forest = mask_forest > 0.0\n",
    "            elif selection=='intact-forests':\n",
    "                mask_forest = data_Hansen.forest_fraction.where(mask_Potapov.Band1==2)\n",
    "                mask_forest = mask_forest > 0.0\n",
    "            elif selection=='all-forests':\n",
    "                mask_forest = data_Hansen.forest_fraction > 0.0\n",
    "\n",
    "            #Create dicts for storing data\n",
    "            SLAND_ctrs = dict()\n",
    "\n",
    "            #Define output file name\n",
    "            fname_out = dir_out + 'SLAND-S2-countries_' + selection + '_Weights-' + data_set + '_MaxForCover' + '{:.2f}'.format(Han2013_thresh) + '_vRemapToForestMask-Hansen2010_IFL_' + year + '.xlsx'\n",
    "            if os.path.exists(fname_out): os.remove(fname_out)\n",
    "\n",
    "            #Create xlsx-file (it will be filled at end of loop with country data from every model)\n",
    "            with pd.ExcelWriter(fname_out) as writer:\n",
    "\n",
    "                #Loop over models\n",
    "                for model in models:\n",
    "\n",
    "                    print(model)\n",
    "\n",
    "                    #Get file name for SLAND\n",
    "                    fnames = [file for file in os.listdir(dir_SLAND) if (model + '_' in file) and (gridsize in file) and ('SLAND_NBP' in file) and ('NBPPFT' not in file)]\n",
    "                    if len(fnames)!=1:  sys.exit('Filename not unique')\n",
    "\n",
    "                    #Read SLAND data\n",
    "                    fname = dir_SLAND + fnames[0]\n",
    "                    data_SLAND = xr.open_dataset(fname)\n",
    "\n",
    "                    #Get lat and lon names\n",
    "                    if 'latitude' in data_SLAND.dims:  lat_name, lon_name = 'latitude', 'longitude'\n",
    "                    else:                              lat_name, lon_name = 'lat', 'lon'\n",
    "\n",
    "                    #Check that model grid and country grid agree\n",
    "                    check_lat1 = np.max(np.abs(data_SLAND[lat_name].values - mask_forest.lat.values))\n",
    "                    check_lon1 = np.max(np.abs(data_SLAND[lon_name].values - mask_forest.lon.values))\n",
    "                    check_lat2 = np.max(np.abs(data_ctrs_ISO.lat.values - mask_forest.lat.values))\n",
    "                    check_lon2 = np.max(np.abs(data_ctrs_ISO.lon.values - mask_forest.lon.values))\n",
    "                    if check_lat1>0.01:  sys.exit('Latitudes do not agree')\n",
    "                    if check_lon1>0.01:  sys.exit('Longitudes do not agree')\n",
    "                    if check_lat2>0.01:  sys.exit('Latitudes do not agree')\n",
    "                    if check_lon2>0.01:  sys.exit('Longitudes do not agree')            \n",
    "\n",
    "                    #Re-index if there are small deviations in lat and lon\n",
    "                    if (check_lat1!=0) or (check_lon1!=0):  data_SLAND    = data_SLAND.reindex({lat_name: mask_forest['lat'], lon_name: mask_forest['lon']}, method='nearest')\n",
    "                    if (check_lat2!=0) or (check_lon2!=0):  data_ctrs_ISO = data_ctrs_ISO.reindex({lat_name: mask_forest['lat'], lon_name: mask_forest['lon']}, method='nearest')\n",
    "\n",
    "                    #Get weights (if no weights for specific model exist, use the average of all other models)\n",
    "                    if data_set=='BLUE':\n",
    "                        weight = for_weight\n",
    "                    else:\n",
    "                        if model in for_weight.model:\n",
    "                            weight = for_weight.sel(model=model)\n",
    "                        else:\n",
    "                            print('Averaging data from other models for ' + model)\n",
    "                            weight = for_weight.mean('model')\n",
    "\n",
    "                    #Apply forest cover thershold\n",
    "                    weight = weight.where(data_Hansen.forest_fraction>Han2013_thresh)                       \n",
    "                        \n",
    "                    #Apply forest mask\n",
    "                    data_SLAND = (data_SLAND * weight).where(mask_forest==1)\n",
    "\n",
    "                    #Loop over all country codes\n",
    "                    for i, iso_alpha3 in enumerate(data_IPCC_codes['ISO']):\n",
    "\n",
    "                        if np.mod(i, 50)==0:\n",
    "                            print('  -run ' + str(i+1) + ' of ' + str(len(data_IPCC_codes['ISO'])))\n",
    "\n",
    "                        #Get numbeic ISO code of country\n",
    "                        iso_numeric = data_alph_num['Numeric'][data_alph_num['Alpha-3 code']==iso_alpha3].values[0]\n",
    "\n",
    "                        #Select country in country mask\n",
    "                        mask_ISO = data_ctrs_ISO.ISOcode==iso_numeric\n",
    "\n",
    "                        #Get SLAND sum in selected country\n",
    "                        data_sel = data_SLAND.where(mask_ISO).sum(('lat', 'lon'))\n",
    "\n",
    "                        #Convert to Tg C/year\n",
    "                        data_sel = 1000 * data_sel\n",
    "\n",
    "                        #Save values in dict\n",
    "                        SLAND_ctrs[iso_alpha3] = data_sel.SLAND.values\n",
    "\n",
    "                    #Special cases for certain IPCC countries\n",
    "                    SLAND_ctrs['SXM'] = SLAND_ctrs['MAF']                                          # Saint Martin is French part of island with Sint Maarten (Dutch part) -> same values are counted for both\n",
    "                    SLAND_ctrs['ANT'] = SLAND_ctrs['BES'] + SLAND_ctrs['CUW'] + SLAND_ctrs['SXM']  # Netherlands Antilles (Bonaire, Saint Eustatius & Saba + Curacao + Sint Maarten)\n",
    "\n",
    "                    #Convert data to data frame (and sort by country name)\n",
    "                    SLAND_ctrs_df = pd.DataFrame(SLAND_ctrs, index=data_SLAND.time.dt.year)\n",
    "                    SLAND_ctrs_df = SLAND_ctrs_df.reindex(sorted(SLAND_ctrs_df.columns), axis=1)\n",
    "\n",
    "                    #Adde units in first cell\n",
    "                    SLAND_ctrs_df = SLAND_ctrs_df.rename_axis('unit: TG C/year')\n",
    "\n",
    "                    #Create sheet in xlsx for every model and store country data\n",
    "                    SLAND_ctrs_df.to_excel(writer, sheet_name=model + '_SLAND_IPCC_ctrs', index=True, header=True, float_format='%.6f')\n",
    "\n",
    "#Remove regridded country mask and regridded forest fraction\n",
    "if os.path.exists(fname_ctrs_reg):  os.remove(fname_ctrs_reg)\n",
    "if os.path.exists(fname_Hansen):    os.remove(fname_Hansen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate total weighted SLAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define models\n",
    "models = ['CABLE-POP', 'CLASSIC', 'CLASSIC-N', 'CLM5.0', 'DLEM', 'IBIS', 'ISAM', 'ISBA-CTRIP', 'JSBACH', 'JULES-ES-1.1',\n",
    "          'LPJ-GUESS', 'LPJwsl', 'LPX-Bern', 'OCN', 'ORCHIDEE', 'ORCHIDEEv3', 'SDGVM', 'VISIT', 'YIBs']\n",
    "\n",
    "#Define grid size\n",
    "gridsize = '360x720-ForestMask'\n",
    "\n",
    "#Select year for forst mask (2000, 2013, or 2016)\n",
    "year = '2013'\n",
    "\n",
    "#Read ISO codes for countries, IPCC countries, and conversions between ISO alpha-3 codes from IPCC and ISO numeric\n",
    "fname_ctrs_ISO   = dir_tmp + 'wrld_cntrs_BLUE_TN_upd_on_ForestMask_grid.nc'\n",
    "fname_IPCC_codes = dir_ctrs + 'IPCC_regions.xlsx'\n",
    "fname_ISO_num    = dir_ctrs + 'iso_codes_alpha_numeric.xlsx'\n",
    "data_ctrs_ISO   = xr.open_dataset(fname_ctrs_ISO)\n",
    "data_IPCC_codes = pd.read_excel(fname_IPCC_codes, sheet_name='region_classification', header=0, usecols=[0, 1, 3])\n",
    "data_alph_num   = pd.read_excel(fname_ISO_num, header=0)\n",
    "\n",
    "#Read weight factors for forest\n",
    "fname_weight = dir_forest + 'Weights_ForestFraction_DGVMs-S2-S3_regrid360x720-ForestMask.nc'\n",
    "for_weight = xr.open_dataset(fname_weight)\n",
    "for_weight = for_weight.w_FF\n",
    "\n",
    "#Re-index data if necessary\n",
    "check_lat2 = np.max(np.abs(for_weight['lat'].values - data_ctrs_ISO.lat.values))\n",
    "check_lon2 = np.max(np.abs(for_weight['lon'].values - data_ctrs_ISO.lon.values))\n",
    "if check_lat1>0.001 or check_lon1>0.001:  sys.exit('Latitudes do not agree')\n",
    "if check_lat2>0.001 or check_lon2>0.001:  sys.exit('Latitudes do not agree')\n",
    "for_weight  = for_weight.reindex({'lat': data_ctrs_ISO['lat'], 'lon': data_ctrs_ISO['lon']}, method='nearest')\n",
    "\n",
    "#Create dicts for storing data\n",
    "SLAND_ctrs = dict()\n",
    "\n",
    "#Define output file name\n",
    "fname_out = dir_out + 'SLAND-S2-countries_Weights-' + data_set + '_total-SLAND_vRemapToForestMask-Hansen2010_IFL_' + year + '.xlsx'\n",
    "if os.path.exists(fname_out): os.remove(fname_out)\n",
    "\n",
    "#Create xlsx-file (it will be filled at end of loop with country data from every model)\n",
    "with pd.ExcelWriter(fname_out) as writer:\n",
    "\n",
    "    #Loop over models\n",
    "    for model in models:\n",
    "\n",
    "        print(model)\n",
    "\n",
    "        #Get file name for SLAND\n",
    "        fnames = [file for file in os.listdir(dir_SLAND) if (model + '_' in file) and (gridsize in file) and ('SLAND_NBP' in file) and ('NBPPFT' not in file)]\n",
    "        if len(fnames)!=1:  sys.exit('Filename not unique')\n",
    "\n",
    "        #Read SLAND data\n",
    "        fname = dir_SLAND + fnames[0]\n",
    "        data_SLAND = xr.open_dataset(fname)\n",
    "\n",
    "        #Get lat and lon names\n",
    "        if 'latitude' in data_SLAND.dims:  lat_name, lon_name = 'latitude', 'longitude'\n",
    "        else:                              lat_name, lon_name = 'lat', 'lon'\n",
    "\n",
    "        #Get weights (if no weights for specific model exist, use the average of all other models)\n",
    "        if model in for_weight.model:\n",
    "            weight = for_weight.sel(model=model)\n",
    "        else:\n",
    "            weight = for_weight.mean('model')\n",
    "\n",
    "        #Define weight for all land and apply weighting\n",
    "        weight = weight.where(weight>0, 1)\n",
    "        data_SLAND = data_SLAND * weight\n",
    "\n",
    "        #Loop over all country codes\n",
    "        for i, iso_alpha3 in enumerate(data_IPCC_codes['ISO']):\n",
    "\n",
    "            if np.mod(i, 50)==0:\n",
    "                print('  -run ' + str(i+1) + ' of ' + str(len(data_IPCC_codes['ISO'])))\n",
    "\n",
    "            #Get numbeic ISO code of country\n",
    "            iso_numeric = data_alph_num['Numeric'][data_alph_num['Alpha-3 code']==iso_alpha3].values[0]\n",
    "\n",
    "            #Select country in country mask\n",
    "            mask_ISO = data_ctrs_ISO.ISOcode==iso_numeric\n",
    "\n",
    "            #Get SLAND sum in selected country\n",
    "            data_sel = data_SLAND.where(mask_ISO).sum(('lat', 'lon'))\n",
    "\n",
    "            #Convert to Tg C/year\n",
    "            data_sel = 1000 * data_sel\n",
    "\n",
    "            #Save values in dict\n",
    "            SLAND_ctrs[iso_alpha3] = data_sel.SLAND.values\n",
    "\n",
    "        #Special cases for certain IPCC countries\n",
    "        SLAND_ctrs['SXM'] = SLAND_ctrs['MAF']                                          # Saint Martin is French part of island with Sint Maarten (Dutch part) -> same values are counted for both\n",
    "        SLAND_ctrs['ANT'] = SLAND_ctrs['BES'] + SLAND_ctrs['CUW'] + SLAND_ctrs['SXM']  # Netherlands Antilles (Bonaire, Saint Eustatius & Saba + Curacao + Sint Maarten)\n",
    "\n",
    "        #Convert data to data frame (and sort by country name)\n",
    "        SLAND_ctrs_df = pd.DataFrame(SLAND_ctrs, index=data_SLAND.time.dt.year)\n",
    "        SLAND_ctrs_df = SLAND_ctrs_df.reindex(sorted(SLAND_ctrs_df.columns), axis=1)\n",
    "\n",
    "        #Adde units in first cell\n",
    "        SLAND_ctrs_df = SLAND_ctrs_df.rename_axis('unit: TG C/year')\n",
    "\n",
    "        #Create sheet in xlsx for every model and store country data\n",
    "        SLAND_ctrs_df.to_excel(writer, sheet_name=model + '_SLAND_IPCC_ctrs', index=True, header=True, float_format='%.6f')\n",
    "\n",
    "#Remove regridded country mask and regridded forest fraction\n",
    "if os.path.exists(fname_ctrs_reg):  os.remove(fname_ctrs_reg)\n",
    "if os.path.exists(fname_Hansen):    os.remove(fname_Hansen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (based on the module python3/2022.01)",
   "language": "python",
   "name": "python3_2022_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
