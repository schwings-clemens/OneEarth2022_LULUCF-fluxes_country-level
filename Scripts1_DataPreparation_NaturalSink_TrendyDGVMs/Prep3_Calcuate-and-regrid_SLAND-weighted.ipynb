{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import cftime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_SLAND    = '/Data/SLAND_Trendy-v10_S2_LatLon/'\n",
    "dir_forest   = '/Data/forest_masks/'\n",
    "dir_area     = '/Data/gridarea/'\n",
    "dir_grids    = '/Data/grids/'\n",
    "dir_tmp      = '/Data/tmp/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regrid and collect weighted SLAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sta = '2001'\n",
    "time_end = '2015'\n",
    "\n",
    "#Define models\n",
    "models = ['CABLE-POP', 'CLASSIC', 'CLM5.0', 'DLEM', 'IBIS', 'ISAM', 'ISBA-CTRIP', 'JSBACH', 'JULES-ES-1.1',\n",
    "          'LPJ-GUESS', 'LPJwsl', 'LPX-Bern', 'OCN', 'ORCHIDEEv3', 'SDGVM', 'VISIT', 'YIBs'] #'CLASSIC-N', 'ORCHIDEE', \n",
    "\n",
    "experiment = 'S2'\n",
    "\n",
    "#Select grid size\n",
    "gridsize = '360x720'\n",
    "gridsize = '360x720-ForestMask'\n",
    "\n",
    "#Define compression for output NetCDF file\n",
    "comp = dict(zlib=True, complevel=3)\n",
    "\n",
    "#File name of grid file\n",
    "file_grid_common = dir_grids + 'grid_xy_' + gridsize\n",
    "\n",
    "#Loop over models\n",
    "create = 1\n",
    "for model in models:\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    #Read SLAND data\n",
    "    fname = dir_SLAND + model + '_' + experiment + '_SLAND_NBP.nc'\n",
    "    data_SLAND = xr.open_dataset(fname)\n",
    "    \n",
    "    #Read area\n",
    "    fname_area = dir_area + 'gridarea_' + model + '.nc'\n",
    "    data_area = xr.open_dataset(fname_area)\n",
    "    \n",
    "    #Select and average over time\n",
    "    data_SLAND = data_SLAND.sel(time=slice(time_sta, time_end))    \n",
    "    data_SLAND = data_SLAND.mean('time')\n",
    "    \n",
    "    #Define file name for forest weights\n",
    "    fname_weight = dir_forest + 'Weights_ForestFraction_DGVMs-S2-S3_grid_' + model + '.nc'\n",
    "    \n",
    "    #File name of grid file\n",
    "    file_grid = dir_grids + 'grid_xy_' + model\n",
    "    \n",
    "    #Distinguish whethera weighing mask exists or not for each model\n",
    "    if os.path.exists(fname_weight):\n",
    "\n",
    "        #Flag for deleting files\n",
    "        del_files = 1\n",
    "\n",
    "        #Define file names for remapping (in case different Trendy versions use different grids)\n",
    "        fname_weight_tmp    = dir_tmp + 'weights_' + model + '_regSLAND_tmp.nc'\n",
    "        fname_weight_regrid = dir_tmp + 'weights_' + model + '_regridded_regSLAND_tmp.nc'\n",
    "        if os.path.exists(fname_weight_tmp):     os.remove(fname_weight_tmp)\n",
    "        if os.path.exists(fname_weight_regrid):  os.remove(fname_weight_regrid)\n",
    "\n",
    "        #Create temporary reference grid and set grid of weighting factor\n",
    "        fname_grid_tmp = dir_tmp + 'grid_xy_regSLAND_tmp_' + model \n",
    "        if os.path.exists(fname_grid_tmp): os.remove(fname_grid_tmp)\n",
    "        os.system('cdo -s griddes -selvar,SLAND ' + fname + ' > ' + fname_grid_tmp)\n",
    "        os.system('cdo -s setgrid,' + fname_grid_tmp + ' ' + fname_weight + ' ' + fname_weight_tmp)\n",
    "\n",
    "        #Remap weighting factors to DGVM grid \n",
    "        os.system('cdo -s remapcon,' + file_grid + ' ' + fname_weight_tmp + ' ' + fname_weight_regrid)\n",
    "\n",
    "        #Read regridded weighting factors\n",
    "        for_weight = xr.open_dataset(fname_weight_regrid)\n",
    "        weight     = for_weight.w_FF\n",
    "\n",
    "    else:\n",
    "\n",
    "        #Flag for deleting files\n",
    "        del_files = 2\n",
    "\n",
    "        #Read weighting factors of all other DGVMs and calculate multi-model mean\n",
    "        fname_weight = dir_forest + 'Weights_ForestFraction_DGVMs-S2-S3_regrid360x720-ForestMask.nc'\n",
    "        for_weight = xr.open_dataset(fname_weight)\n",
    "        for_weight = for_weight.mean('model')\n",
    "\n",
    "        #Define file names for remapping\n",
    "        fname_weight_tmp1   = dir_tmp + 'weights_' + model + '_regSLAND_tmp1.nc'\n",
    "        fname_weight_tmp2   = dir_tmp + 'weights_' + model + '_regSLAND_tmp2.nc'\n",
    "        fname_weight_regrid = dir_tmp + 'weights_' + model + '_regridded_regSLAND_tmp.nc'\n",
    "        if os.path.exists(fname_weight_tmp1):    os.remove(fname_weight_tmp1)\n",
    "        if os.path.exists(fname_weight_tmp2):    os.remove(fname_weight_tmp2)\n",
    "        if os.path.exists(fname_weight_regrid):  os.remove(fname_weight_regrid)\n",
    "\n",
    "        #Save multi-model mean as NetCDF and set correct grid\n",
    "        for_weight.to_netcdf(fname_weight_tmp1)\n",
    "        file_grid_FM = dir_grids  + 'grid_xy_360x720'\n",
    "        os.system('cdo -s setgrid,' + file_grid_FM + ' ' + fname_weight_tmp1 + ' ' + fname_weight_tmp2)\n",
    "\n",
    "        #Remap weighting factors to model grid\n",
    "        os.system('cdo -s remapcon,' + file_grid + ' ' + fname_weight_tmp2 + ' ' + fname_weight_regrid)\n",
    "\n",
    "        #Read regridded weighting factors\n",
    "        for_weight = xr.open_dataset(fname_weight_regrid)\n",
    "        weight     = for_weight.w_FF\n",
    "\n",
    "    #Define coorcinate names\n",
    "    if 'latitude' in data_SLAND.coords:  lat_name, lon_name = 'latitude', 'longitude'\n",
    "    else:                          lat_name, lon_name = 'lat', 'lon'\n",
    "        \n",
    "    #Re-index data if necessary\n",
    "    check_lat1 = np.max(np.abs(weight[lat_name].values - data_SLAND[lat_name].values))\n",
    "    check_lon1 = np.max(np.abs(weight[lon_name].values - data_SLAND[lon_name].values))\n",
    "    check_lat2 = np.max(np.abs(data_area[lat_name].values - data_SLAND[lat_name].values))\n",
    "    check_lon2 = np.max(np.abs(data_area[lon_name].values - data_SLAND[lon_name].values))    \n",
    "    if check_lat1>0.001 or check_lon1>0.001:  sys.exit('Coordinates do not agree')\n",
    "    if check_lat2>0.001 or check_lon2>0.001:  sys.exit('Coordinates do not agree')\n",
    "    weight    = weight.reindex({lat_name: data_SLAND[lat_name], lon_name: data_SLAND[lon_name]}, method='nearest')\n",
    "    data_area = data_area.reindex({lat_name: data_SLAND[lat_name], lon_name: data_SLAND[lon_name]}, method='nearest')\n",
    "\n",
    "    #Define weight for all land and apply weighting\n",
    "    weight = weight.where(weight>0, 1)\n",
    "    \n",
    "    #Calculate weighted SLAND density\n",
    "    data_SLAND = data_SLAND * weight / data_area.cell_area   \n",
    "    \n",
    "    #Define file names for regridding\n",
    "    fname_tmp      = dir_tmp + 'SLAND_' + model + '_tmp.nc'\n",
    "    fname_tmp_regr = dir_tmp + 'SLAND_' + model + '_tmp_regr.nc'\n",
    "    if os.path.exists(fname_tmp):       os.remove(fname_tmp)\n",
    "    if os.path.exists(fname_tmp_regr):  os.remove(fname_tmp_regr)\n",
    "\n",
    "    #Save SLAND density\n",
    "    data_SLAND.to_netcdf(fname_tmp)\n",
    "    \n",
    "    #Remap SLAND to common grid\n",
    "    os.system('cdo -s remapcon,' + file_grid_common + ' ' + fname_tmp + ' ' + fname_tmp_regr)    \n",
    "\n",
    "    #Read regridded data\n",
    "    data_SLAND_regr = xr.open_dataset(fname_tmp_regr).load()\n",
    "    \n",
    "    #Collect in array\n",
    "    if create==1:\n",
    "        data_coll = data_SLAND_regr\n",
    "        create = 0\n",
    "    else:\n",
    "        data_coll = xr.concat((data_coll, data_SLAND_regr), dim='model')\n",
    "\n",
    "    #Remove temporary files\n",
    "    os.remove(fname_tmp)\n",
    "    os.remove(fname_tmp_regr)\n",
    "    if del_files==1:\n",
    "        os.remove(fname_grid_tmp)\n",
    "        os.remove(fname_weight_tmp)\n",
    "        os.remove(fname_weight_regrid)\n",
    "    elif del_files==2:        \n",
    "        os.remove(fname_weight_tmp1)\n",
    "        os.remove(fname_weight_tmp2)\n",
    "        os.remove(fname_weight_regrid)\n",
    "        \n",
    "#Add model names\n",
    "data_coll['model'] = models\n",
    "\n",
    "#Calculate flux in Pg C / ha / year ([data_coll] =  Pg C / m2 / year)\n",
    "data_coll_dens = 10**4 * data_coll\n",
    "\n",
    "#Convert Pg C / ha / year to t C / ha / year\n",
    "data_coll_dens = 10**9 * data_coll_dens\n",
    "\n",
    "#Set attributes\n",
    "data_coll_dens['SLAND'].attrs['standard_name'] = 'Natural land C sink'\n",
    "data_coll_dens['SLAND'].attrs['long_name']     = 'natural_land_C_sink'\n",
    "data_coll_dens['SLAND'].attrs['units']         = 't C / ha / year'\n",
    "\n",
    "#Save as NetCDF\n",
    "fname_out = dir_SLAND + \"SLAND-weighted_ForestFraction_DGVMs-S2-S3_all-models_\" + time_sta + \"-\" + time_end + \"-mean_regrid\" + gridsize + \".nc\"\n",
    "if os.path.exists(fname_out):  os.remove(fname_out)\n",
    "data_coll_dens.to_netcdf(fname_out, encoding={var: comp for var in data_coll_dens.data_vars})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (based on the module python3/2022.01)",
   "language": "python",
   "name": "python3_2022_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
